---
title: "Perceptions of Security in Mexico"
subtitle: "HarvardX PH125.9x Capstone"
author: "Octavio Rodríguez Ferreira"
date: "6/10/2021"
output: 
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading, include=FALSE}
##########################################################
#                                                        #
#             LOADING DATA AND PACKAGES                  #
#                                                        #
##########################################################

#########################################################
##############   PACKAGES AND LIBRARIES    ##############
#########################################################

#Downloading packages if required
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(devtools)) install.packages("devtools", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")


#Loading package libraries
library(readxl)
library(tidyverse)
library(caret)
library(data.table)
library(dplyr)
library(ggplot2)
library(devtools)
library(gridExtra)
library(ggpubr)

#########################################################
###############      LOADING DATA        ################
#########################################################

#Load data
envipe <- read_csv("~/InSync/Up.Edu.Mx/02-ORODs/01-ACADEMIC/03-LEARNING/2020_EdX_R/9.CAPSTONE/CYO_Project/data/conjunto_de_datos_envipe2020_csv/conjunto_de_datos_TPer_Vic1_ENVIPE_2020/conjunto_de_datos/conjunto_de_datos_TPer_Vic1_ENVIPE_2020.csv")

## Data on perception of security from Mexico's 2020 National Survey of Crime, Victimization and Public Security Perceptions (Encuesta Nacional de Victimización y Percepción sobre Seguridad Pública, ENVIPE), from the National Institute of Geography and Statistics (Instituto Nacional de Geografía y Estadística, INEGI) available at <https://www.inegi.org.mx/contenidos/programas/envipe/2020/datosabiertos/conjunto_de_datos_envipe2020_csv.zip>

##Downloading data was done with the `read_csv` function because it couldn't be easily downloaded from the website. I downloaded the data manually, looked for the relevant file (there are a lot of different files in the downloaded file), and use the function to download it to RStudio.

##The path to this data set once the full file is downloaded should be: conjunto_de_datos_envipe2020_csv/conjunto_de_datos_TPer_Vic1_ENVIPE_2020/conjunto_de_datos/conjunto_de_datos_TPer_Vic1_ENVIPE_2020.csv
```

# Executive summary

This project intends to determine whether trends on crime and violence are the only factors influencing perceptions of security, or if other more nuanced factors also have an important wight in shaping such perceptions. We argue that perceptions of security, while certainly determined by "hard" data such as crime, victimization and violence; are indeed influenced by other factors, and that they have a different impact across segments of population. Far from trying to find those "unknown" more nuanced factors, this project aims to prove how those factors influence different groups. Through different machine learning analysis, we predict almost 70% of responses to a Mexican public security perceptions survey, only testing certain variables such as gender, age, type of area of residence (rural v. urban), SES, and the municipality of residence. Thus we conclude that trends on crime and violence are not the only factors influencing perceptions of security, and that there are other factors that influence differently various population groups.

# Introduction

Mexico is considered one of the most violent countries in the world. Over the last fifteen years, there has been a steady increase in intentional homicides and other violent crimes. Violence in Mexico has have record-high numbers since 2017 and a steady increase since 2015, this measured by intentional homicides, which is a proxy to measure violent crime and a good indicator of levels of security (UNODC 2014) and “instrumental violence” (van Dijk 2008, 157). In 2020, amid the COVID-19 pandemic, crime and murder rates appeared to have leveled off, but remained at historically high levels.

The toll that the increase in violence and in violent crimes has taken on Mexican society has been evident. Since 2015, public's negative perception of security [hereinafter POS] has been a constant. According to several surveys by Mexico's National Institute for Statistics and Geography (Instituto Nacional de Estadística y Geografía, INEGI), the percentage of Mexicans who feel safe living in their respective municipality or city is only about 30% with minor fluctuations year by year, which means that around two thirds of the Mexican population feel unsafe living in their municipality or city (Justice in Mexico, 2016).

However, while perceptions of insecurity in Mexican municipalities have increased from 63% in 2013 to 70% in 2019, the rate of crime victimization has stayed fairly stable, and even decreased in 2018 and 2019, according to INEGI's National Survey of Crime, Victimization and Public Security Perceptions (Encuesta Nacional de Victimización y Percepción sobre Seguridad Pública, ENVIPE).

```{r vic_insec, echo=FALSE}

#This section is only for the introduction section of the final report.
#Data used for graphs come from the same survey, but from a different data base.
#We decided to input the values directly into the plot without loading an entire other data set to avoid confusion.
#The numbers for the following plots were obtained in the Executive Summary of the survey, available here:  <<https://www.inegi.org.mx/contenidos/programas/envipe/2019/doc/envipe2019_presentacion_nacional.pdf>>

#Values
Years <- c(2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)
Rates <- c(27337, 28224, 28200, 28202, 28788, 29746, 28269, 24849)
Percentages <- c(63, 64.2, 64.1, 65.1, 66.3, 70, 70.5, 70.3)

#Crime victimization rate plot
rteplot <- ggplot() +
  geom_bar(aes(x=Years, y=Rates),stat="identity", fill="gray", color="black") +
  ggtitle("Crime victimization rate")

#Percentage of perceptions of insecurity
perplot <- ggplot() +
  geom_line(aes(x = Years, y= Percentages), color = "red") +
  scale_y_continuous(limits = c(60,75)) +
  ggtitle("Perceptions of insecurity(%)")

#Grid 
grid.arrange(rteplot, perplot, 
             ncol = 2,
             top = "Crime victimization v. Perceptions of insecurity")
```

So the question arises if the POS is exclusively influenced by crime and violence rates, or if there other factors that help to shape own perceptions of insecurity. In other words, can one feel unsafe without having being a victim of crime? or, better yet, if increases in perception of insecurity necessarily reflect increases in actual victimization. 

Our hypothesis is, no. We believe there are other more nuanced factors that have an important weight in shaping such perceptions. We do not intend, however, to find those factors, but to determine how certain "hidden" causes are relevant in own perceptions of security, and how those causes affect differently, different population groups. Overall, we argue that only by considering a handful of sociodemographic variables, we can very accurately predict perceptions of security and insecurity, which would be an indication of the existence of those "hidden" factors that influence such perceptions.

In order to achieve this, we use data from INEGI's ENVIPE survey from 2020, where we use variables such as age, gender, SES, municipality and the type of area where the respondents live (rural, urban or suburban) and a response to a question on whether respondents perceive their municipality as secure or insecure.

We use machine learning algorithms such as logistic regression, naive bayes and random forest to try to predict responses on POS using the sociodemographic variables mentioned above. With our most successful model, we can predict almost 70% of such responses.

We acknowledge that an accuracy of 67% is not as strong statistically, but it does help to demonstrate our hypothesis, since our model was able to predict respondent's perceptions in the majority of cases (2 out of 3 times), by testing simple sociodemographic indicators, and without without incorporating other variables that could be more influential in such a measure (crime and victimization rates, etc.).

Thus we conclude that our model is solid enough to demonstrate that trends on crime and violence are not the only factors influencing perceptions of security, and that we can infer the existence of other "hidden" determinants that relate to different characteristics of the respondent as part of a social group.

# Initial Exploration

## Data features

The original ENVIPE data set has several problems that had to be addressed. It has several columns, most of which we won't use, the column names are illegible, there are blank spaces within values, all columns are of a `character` class, and "No Response" was coded with the number `9`. So we took following steps to clean the data:

* Remove all blank spaces
```{r blank_sp, include=FALSE}
#Remove spaces
envipe_clean <- as.data.frame(apply(envipe,2,function(x)gsub('\\s+', '',x)))
```
* Delete unnecessary columns
```{r rmv_cols, include=FALSE}
###########  SUBSET OF VARIABLES OF INTEREST  ###########

#Check variables (column names)
colnames(envipe_clean)

#Select only the variables we're interested in. Also, change the order of some columns for better understanding.
my_vars <- c("ID_PER",
             "SEXO",
             "EDAD",
             "ESTRATO",
             "CVE_ENT",
             "CVE_MUN",
             "DOMINIO",
             "AP4_3_2")

##Variables of interest are: id, sex, age, socioeconomic strata, state code, municipality code, type of area of residence (rural vs. urban), and responses about perception of security in their municipality (pos).

#Create a new data frame with variables of interest
pos <- envipe_clean[,my_vars]

#Remove unnecessary objects
rm(my_vars)
```
* Change classes
```{r chng_class, include=FALSE}
##Leave `CVE_ENT` and `CVE_MUN` (state and municipality code) as character to keep leading zeroes.

#Change to numeric
pos[,2:4] <- apply(pos[,2:4], 
                   2, function(x) as.numeric(as.character(x)))

#Re code the area variable `DOMINIO` to numeric [Urban(U) = 1, Suburban (C) =  2, Rural (R) = 3]
pos$DOMINIO <- as.factor(gsub("U", 1, gsub("C", 2, gsub("R", 3, pos$DOMINIO))))

#Relocate next to other predictors
pos <- pos %>% relocate(DOMINIO, .after = ESTRATO)

#Define dependent variable `AP4_3_2` that measures for perception of security in municipality (pos) as factor
pos$AP4_3_2 <- as.factor(pos$AP4_3_2)
```
* Create a unique number for the municipality of respondent
```{r mun_var, include=FALSE}
#############   CREATE LOCATION VARIABLE   ##############

#Creating a unique number for the municipality of respondent
#Convert `CVE_ENT` (State) and `CVE_MUN` (Municipality) into `mun`, a new variable for specific location that includes both the state and municipality codes.
pos$mun <- paste(pos$CVE_ENT, pos$CVE_MUN)

#Remove spaces
pos$mun <- gsub('\\s+', '', pos$mun)

#Relocate new `mun` column
pos <- pos %>% relocate(mun, .after = CVE_MUN)

#Change to numeric
pos$mun <- as.numeric(pos$mun)

#Remove unnecessary columns
pos <- pos[,-c(6:7)]
```
* Recode the existing variables
```{r recod_var, include=FALSE}
###############     RECODE  VARIABLES     ###############

#New column names
nu_colnames <- c("id",
                 "sex",
                 "age",
                 "ses",
                 "area",
                 "mun",
                 "pos")

#Rename columns
colnames(pos) <- nu_colnames

#Remove unnecessary object
rm(nu_colnames)
```
* Remove the "No Response" coded as "9" from our dependent variable (POS)
```{r no_resp, include=FALSE}
#Subset and drop levels (Drop "9" that codes 'No Response')
pos <- subset(pos, pos != "9")
pos$pos<- droplevels(pos)$pos
```
* Add a new variable for "age group" to see if it performs better than the "age" variable that is a continuous value.
```{r age_grp, include=FALSE}
#Create new variable of 'age groups' as categorical variable
pos <- pos %>% 
  mutate(age_grp = 
           cut(age, c(10,19,39,59,79,100),
               labels = c("-20", "20–39", "40–59", "60-79", "80+")))

#Relocate
pos <- pos %>% relocate(age_grp, .after = age)
```

With all the changes described above, we end up with a data set of `89171` observations and `8` variables.

```{r var_desc, echo=FALSE}

############       VARIABLE DESCRIPTION       ############

#Descriptions of variables
var_desc <- c("Id of respondent",
              "Sex of respondent",
              "Age of respondent",
              "Age group of respondent",
              "Socio-economic strata",
              "Type of area of residence",
              "Unique code for municipality of residence",
              "Perception of security in the municipality")

#Values of variables
var_vals <- c("Unique number",
              "Male = 1, Female =2",
              "11 to 80",
              "-20, 20–39, 40–59, 60-79, 80+",
              "Low = 1, Medium-low = 2, Medium-high = 3, High = 4",
              "Urban = 1, Suburban =  2, Rural = 3",
              "Code composed of State and Municipality official numbers", 
              "1 = Secure, 2 = Insecure")

#Classes of variables
envipe_vars <- (sapply(lapply(pos, class), "[", 1))

#Table of variables
vars_explained <- data.frame(envipe_vars, var_desc, var_vals)

#Column names for table
colnames(vars_explained) <- c("Class", "Variable description", "Values")

#Remove unnecessary objects
rm(var_desc, var_vals, envipe_vars)

#Table of variables
knitr::kable(vars_explained)

```

## Descriptive Analysis

The distribution of respondents to the ENVIPE survey is fairly well distributed geographically. That means we can have perspectives from people in different contexts and with different backgrounds.

```{r map, message=FALSE, warning=FALSE, echo=FALSE}
##########  MAP OF RESPONDENTS PER MUNICIPALITY ##########

#Packages and data
## Download a map of Mexican municipalities
devtools::install_github("diegovalle/mxmaps", force = TRUE)
library("mxmaps")

#Load data for a municipal level map of Mexico
data("df_mxmunicipio")

#Modify column from map to match values from data set
mapmx <- df_mxmunicipio %>%
  mutate(mun = as.numeric(region)) %>%
  relocate(mun, .after = state_name)

#Merge our data set into map data 
mapmx <- left_join(mapmx, pos, by = "mun")

#Remove unnecessary columns
mapmx <- mapmx[,-c(1:4, 6:18)]

#Rename columns 
## The function "mxmunicipio_choropleth" from <https://www.diegovalle.net/mxmaps/index.html> included in the map package allows you to create GIS maps.
##"The data.frame that you provide to the plotting functions must have one column named 'region' with the INEGI codes of the states or municipios and another one named 'value' with the values to plot."
##The `region` variable of the function is exactly the same as our `mun` variable. 
colnames(mapmx)[colnames(mapmx) == "mun"] <- "region"

# Create the map for plotting
mapmx_df <- mapmx %>%
  select(region)  %>%
  group_by(region) %>%
  summarise(value = n())

#Generate map
pos_map <- mxmunicipio_choropleth(mapmx_df, 
                       num_colors = 9,
                       title = "Geolocation of respondents",
                       legend = "Observations")

#Remove unnecessary objects
rm(df_mxmunicipio, mapmx, mapmx_df)

#Plot map
pos_map
```

From an exploratory analysis of the data, we found that almost twice as much of the respondents feel their municipality to be insecure, and women tend to feel less secure than men. In terms of the sociodemographic characteristics, the majority of respondents is younger than 50 years old, mostly between 30 and 50 years old. Also, the vast majority of respondents are middle class (low-middle and high-middle), though the distribution is skewed toward medium-lower and lower class. Finally, the vast majority of respondents come from a urban area and those who come from a sub-urban and rural areas are almost equally distributed.

```{r dists, echo=FALSE}
##############         DISTRIBUTIONS        ##############

#Distribution of opinions on perception of security
h1 <- ggplot(pos, aes(pos)) +
  geom_bar(width = 1,
           fill = "#E55451",
           color = "#800517") +
  scale_x_discrete(labels = c("1" = "Secure", "2" = "Insecure")) +
  xlab("POS") +
  ylab("Observations")

#Distribution of sex of respondents
h2 <- ggplot(pos, aes(as.factor(sex))) +
  geom_bar(width = 1,
           fill = "#8467D7",
           color = "#461B7E") +
  scale_x_discrete(labels = c("1" = "Male", "2" = "Female")) +
  xlab("Sex") +
  ylab("Observations")

#Distribution of age of respondents
h3 <- ggplot(pos, aes(age)) +
  geom_histogram(bins = 5,
                 fill = "#82CAFF",
                 color = "#0C090A") +
  xlab("Age") +
  ylab("Observations")

#Distribution of socio-economic strata
h4<- ggplot(pos, aes(as.factor(ses))) +
  geom_bar(width = 1,
           fill = "#728C00",
           color = "#0C090A") +
  scale_x_discrete(labels = c("1" = "Low", "2" = "Medium-low", "3" = "Medium-high", "4" = "High")) +
  xlab("SES") +
  ylab("Observations")

#Distribution of area
h5 <- ggplot(pos, aes(area)) + 
  geom_bar(width = 1,
           fill = "#E69F00",
           color = "#0C090A") +
  scale_x_discrete(labels = c("1" = "Urban", "2" = "Suburban", "3" = "Rural")) +
  xlab("Area") +
  ylab("Observations")

#Grid of distribution plots
grid.arrange(heights = 2,
             h1 + theme(axis.text.x = element_text(angle = 90, hjust = 1)),
             h2 + theme(axis.text.x = element_text(angle = 90, hjust = 1)),
             h3 + theme(axis.text.x = element_text(angle = 90, hjust = 1)),
             h4 + theme(axis.text.x = element_text(angle = 90, hjust = 1)),
             h5 + theme(axis.text.x = element_text(angle = 90, hjust = 1)))
```

When combined, we can see how different variables interact with each other. For example, on average people from urban areas tend to feel more secure, however younger women from rural areas tend to feel more insecure while older men tend to feel more secure. In urban areas men also feel safer than women, but older men tend to feel more insecure than younger men. In urban areas, the distribution of women who feel either secure or insecure by age is fairly similar. In suburban areas we see almost the same patterns as in urban areas.

```{r per_per_var, echo=FALSE}
#Age, sex, pos, area
f1 <- ggplot(pos, aes(x = as.factor(sex), y = age, fill = as.factor(pos))) + 
  geom_boxplot() +
  facet_wrap(~ area, labeller = as_labeller(c("1" = "Urban", "2" = "Suburban", "3" = "Rural"))) +
  xlab("Sex") +
  ylab("Age") +
  scale_x_discrete(labels = c("1" = "Male", "2" = "Female")) +
  scale_fill_discrete(name="POS", labels= c("1" = "Secure", "2" = "Insecure")) +
  ggtitle("Area") +
  theme(plot.title = element_text(hjust = 0.5, size = 11))

#Plot
f1
```

In regards on how the variables distribute based on SES, we have that the lower class tends to feel more secure as higher classes. Also in lower classes, younger women tend to feel more insecure that men, however those women who say that they feel secure are very balanced with the ones that feel insecure. In medium classes we see a more diverse distribution than with high and low classes. For example, the distribution between those who women who feel secure and insecure is almost the same while older men from the same social class tend to feel more insecure. Though, We see very similar patterns in the both strata of the middle class.

```{r per_per_var2, echo=FALSE}
#Age, sex, pos, ses
#Age, sex, pos, ses
f2 <- ggplot(pos, aes(x = as.factor(sex), y = age, fill = as.factor(pos))) + 
  geom_boxplot() +
  facet_wrap(~ ses, ncol = 4, labeller = as_labeller(c("1" = "Low", "2" = "Medium-low", "3" = "Medium-high", "4" = "High"))) +
  xlab("Sex") +
  ylab("Age") +
  scale_x_discrete(labels = c("1" = "Male", "2" = "Female")) +
  scale_fill_discrete(name="Perception of security", labels= c("1" = "Secure", "2" = "Insecure")) +
  ggtitle("SES") +
  theme(plot.title = element_text(hjust = 0.5, size = 11))

#Plot
f2
```

# Methods and analysis

Before starting the modeling stage, the data needed to be partitioned. To perform a machine learning analysis, it is necessary to divide our data set in two different sets, one for training our algorithms and one for testing. 

## Data partitions

The initial partition of our data consisted of 80% of the original observations as our `training` set, and the `validation` set the remaining 20%, to test the final model. We made an additional partition splitting in half the `training` set into a `train_set` and a `test_set`, to train and test different models much faster and easier.

```{r partitions, include=FALSE}
#########################################################
#########    TRAINING AND VALIDATION SETS    ############
#########################################################

# Creating test and validation sets for  data
set.seed(1982, sample.kind="Rounding") 

#Create index and train and test sets
test_index <- createDataPartition(y = pos$pos, times = 1, p = 0.2, list = FALSE)
training <- pos %>% slice(-test_index)
validation <- pos %>% slice(test_index)

#Remove all temporary elements
rm(test_index)

#########################################################
#########       TRAIN AND TEST SUBSETS       ############
#########################################################

#Create a smaller subset to initial model testing.

# Creating test and validation sets for data
set.seed(1982, sample.kind="Rounding") 

#Creaete index and train and test sets
index <- createDataPartition(y = training$pos, times = 1, p = 0.5, list = FALSE)
train_set <- training %>% slice(-index)
test_set <- training %>% slice(index)

#Remove all temporary elements
rm(index)
```

## Modeling

The first step of our modeling stage was to determine the type of analysis and variables to use. In this stage we used logistic regression model and tested different variables to determine the best combination of predictors.

```{r var_test, include=FALSE}

############        INITIAL GLM MODEL      ##############

#Initial model tries to test if sex, age and municipality of residents are strong enough values to predict perceptions of security.
#The selected baseline method is Logistic Regression. 

#Train model
initial_bl <- train(pos ~ 
                     sex +
                     age +
                     mun,
                   method = "glm",
                   data = train_set)

#Predict
ibl_pred <- predict(initial_bl, test_set)

#Extract accuracy
ibl_acc <- confusionMatrix(data = ibl_pred, reference = test_set$pos)$overall["Accuracy"]

#Initial Baseline accuracy
ibl_acc

## Accuracy 
##0.6261915 

############         VARIABLE TESTING      ##############

#Build accuracy table by variable
vartest_acc <- tibble(Predictors="Sex+Age+Municipality",
                      Accuracy = ibl_acc)

#Test socioeconomic strata (Test 1)

#Train model
vartest_1 <- train(pos ~ 
                       sex +
                       age +
                       mun +
                       ses,
                     method = "glm",
                     data = train_set)

#Predict
vartest_1_pred <- predict(vartest_1, test_set)

#Extract accuracy
vartest_1_acc <- confusionMatrix(data = vartest_1_pred, reference = test_set$pos)$overall["Accuracy"]

#Accuracy table by variable
vartest_acc <- bind_rows(vartest_acc, 
                          tibble(Predictors="Sex+Age+Municipality+SES",
                                 Accuracy = vartest_1_acc))

#Test area (Test 2)

#Train model
vartest_2 <- train(pos ~ 
                     sex +
                     age +
                     mun +
                     area,
                   method = "glm",
                   data = train_set)

#Predict
vartest_2_pred <- predict(vartest_2, test_set)

#Extract accuracy
vartest_2_acc <- confusionMatrix(data = vartest_2_pred, reference = test_set$pos)$overall["Accuracy"]

#Accuracy table by variable
vartest_acc <- bind_rows(vartest_acc, 
                         tibble(Predictors="Sex+Age+Municipality+Area",
                                Accuracy = vartest_2_acc))

#Test area + socioeconomic strata (Test 3)

#Train model
vartest_3 <- train(pos ~ 
                     sex +
                     age +
                     mun +
                     ses +
                     area,
                   method = "glm",
                   data = train_set)

#Predict
vartest_3_pred <- predict(vartest_3, test_set)

#Extract accuracy
vartest_3_acc <- confusionMatrix(data = vartest_3_pred, reference = test_set$pos)$overall["Accuracy"]

#Accuracy table by variable
vartest_acc <- bind_rows(vartest_acc, 
                         tibble(Predictors="Sex+Age+Municipality+SES+Area",
                                Accuracy = vartest_3_acc))

```

The baseline model was then defined as a logistic regression analysis that included the variables `sex`, `age`, `mun`, `ses`, and `area` as predictors, with an initial accuracy of roughly `0.628`.

```{r var_test_res, echo=FALSE}
#Results
vartest_acc %>% knitr::kable()
```

Once defined the variables and the logistic regression as our baseline analysis we tried also a naive bayes and a random forests analysis. We also alternated the use of the `age` and `age_grp` variables with two of the models to see which one yielded a better accuracy.

### Logistic Regressions

In the first logistic regression model we used the `age` variable obtaining an accuracy of almost `0.628`.

```{r bl, include=FALSE}
############      FINAL BASELINE MODEL     ##############

#Train model
fit_bl <- train(pos ~ 
                  sex +
                  age +
                  mun +
                  ses +
                  area,
                method = "glm",
                data = train_set)

#Predict
pred_bl <- predict(fit_bl, test_set)

#Confusion matrix
cm_bl <- confusionMatrix(data = pred_bl, reference = test_set$pos)

#Extract accuracy
acc_bl <- confusionMatrix(data = pred_bl, reference = test_set$pos)$overall["Accuracy"]

#Modeling accuracy table
at_modeling <- tibble(Method="Baseline Logistic Regression",
                      Accuracy = acc_bl)
```

```{r bl_tab, echo=FALSE}
#Table of confusion matrix statistics
knitr::kable(as.data.frame(t(cm_bl$overall)))
```

```{r bl_plot, echo=FALSE}
#Plot of confusion matrix table
cm_bl_tab <- as.data.frame(cm_bl$table) %>%
  ggplot(aes(Reference, Prediction, fill= Freq)) + 
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1) +
  scale_fill_gradient2(low = "#E7E7E7",
                       mid = "#94B447",
                       high = "#003041") +
  geom_text(aes(label = Freq), color = "white", size = 8) +
  guides(fill = guide_colorbar(barwidth = 0.5,
                                barheight = 10))
#Plot
cm_bl_tab

```

We then tried an alternative version of the same baseline model using `age_grp` instead of `age`. The adjusted logistic regression model yielded an accuracy of `0.6285`, which was improvement from the the previous model.

```{r abl, include=FALSE}
#########################################################
########   ADJUSTED LOGISTIC REGRESSION MODEL    ########
#########################################################

#Test age group instead of age

#Train model
fit_abl <- train(pos ~ 
                   sex +
                   age_grp +
                   mun +
                   ses +
                   area,
                 method = "glm",
                 data = train_set)

#Predict
pred_abl <- predict(fit_abl, test_set)

#Confusion matrix
cm_abl <- confusionMatrix(data = pred_abl, reference = test_set$pos)

#Extract accuracy
acc_abl <- confusionMatrix(data = pred_abl, reference = test_set$pos)$overall["Accuracy"]

#Accuracy modeling table
at_modeling <- bind_rows(at_modeling, 
                         tibble(Method="Adjusted Logistic Regression",
                                Accuracy = acc_abl))

```

```{r abl_tab, echo=FALSE}
#Table of confusion matrix statistics
knitr::kable(as.data.frame(t(cm_abl$overall)))
```

```{r abl_plot, echo=FALSE}
#Plot of confusion matrix table
cm_abl_tab <- as.data.frame(cm_abl$table) %>%
  ggplot(aes(Reference, Prediction, fill= Freq)) + 
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1) +
  scale_fill_gradient2(low = "#E7E7E7",
                       mid = "#94B447",
                       high = "#003041") +
  geom_text(aes(label = Freq), color = "white", size = 10) +
  guides(fill = guide_colorbar(barwidth = 0.5,
                               barheight = 10))
#Plot
cm_abl_tab
```

### Naive Bayes

The Naive Bayes algorithm proved to be more accurate than the logistic regression. When trained with `age` variable it gave almost `0.63`. With Naive Bayes we didn't test `age_grp` because the type of variable was not compatible with the algorithm.

```{r nb, include=FALSE}
#########################################################
#############       NAIVE BAYES MODEL       #############
#########################################################

#Train model
fit_nb <- train(pos ~ 
                  sex +
                  age +
                  mun +
                  ses +
                  area,
                method = "nb",
                data = train_set)

#Predict
pred_nb <- predict(fit_nb, test_set)

#Confusion matrix
cm_nb <- confusionMatrix(data = pred_nb, reference = test_set$pos)


#Extract accuracy
acc_nb <- confusionMatrix(data = pred_nb, reference = test_set$pos)$overall["Accuracy"]

#Accuracy modeling table
at_modeling <- bind_rows(at_modeling, 
                         tibble(Method="Naive Bayes",
                                Accuracy = acc_nb))
```

```{r nb_tab, echo=FALSE}
#Table of confusion matrix statistics
knitr::kable(as.data.frame(t(cm_nb$overall)))
```

```{r nb_plot, echo=FALSE}
#Plot of confusion matrix table
cm_nb_tab <- as.data.frame(cm_nb$table) %>%
  ggplot(aes(Reference, Prediction, fill= Freq)) + 
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1) +
  scale_fill_gradient2(low = "#E7E7E7",
                       mid = "#94B447",
                       high = "#003041") +
  geom_text(aes(label = Freq), color = "white", size = 10) +
  guides(fill = guide_colorbar(barwidth = 0.5,
                               barheight = 10))
#Plot
cm_nb_tab
```

### Random Forests

The Random Forests model proved to be the most effective algorithm in the training stage increasing our accuracy to roughly `0.652` using the `age` variable.

```{r rf, include=FALSE}
#########################################################
#############      RANDOM FORESTS  MODEL    #############
#########################################################

#Train model
fit_rf <- train(pos ~ 
                   sex +
                   age +
                   mun +
                   ses +
                   area,
                 method = "rf",
                 data = train_set)

#Predict
pred_rf <- predict(fit_rf, test_set)

#Confusion matrix
cm_rf <- confusionMatrix(data = pred_rf, reference = test_set$pos)

#Extract accuracy
acc_rf <- confusionMatrix(data = pred_rf, reference = test_set$pos)$overall["Accuracy"]

#Accuracy table
at_modeling <- bind_rows(at_modeling, 
                         tibble(Method="Random Forests",
                                Accuracy = acc_rf))
```

```{r rm_tab, echo=FALSE}
#Table of confusion matrix statistics
knitr::kable(as.data.frame(t(cm_rf$overall)))
```

```{r rm_plot, echo=FALSE}
#Plot of confusion matrix table
cm_rf_tab <- as.data.frame(cm_rf$table) %>%
  ggplot(aes(Reference, Prediction, fill= Freq)) + 
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1) +
  scale_fill_gradient2(low = "#E7E7E7",
                       mid = "#94B447",
                       high = "#003041") +
  geom_text(aes(label = Freq), color = "white", size = 10) +
  guides(fill = guide_colorbar(barwidth = 0.5,
                               barheight = 10))
#Plot
cm_rf_tab
```

The algorithm performed even better with the `age_grp` variable, giving us an accuracy of a little over `066`.

```{r arf, include=FALSE}
#########################################################
###########         ADJUSTED RF  MODEL        ###########
#########################################################

#Age group instead of age

#Train model
fit_arf <- train(pos ~ 
                  sex +
                  age_grp +
                  mun +
                  ses +
                  area,
                method = "rf",
                data = train_set)

#Predict
pred_arf <- predict(fit_arf, test_set)

#Confussion matrix
cm_arf <- confusionMatrix(data = pred_arf, reference = test_set$pos)

#Extract accuracy
acc_arf <- confusionMatrix(data = pred_arf, reference = test_set$pos)$overall["Accuracy"]

#Accuracy table
at_modeling <- bind_rows(at_modeling, 
                         tibble(Method="Adjusted Random Forests",
                                Accuracy = acc_arf))
```

```{r arf_tab, echo=FALSE}
#Table of confusion matrix statistics
knitr::kable(as.data.frame(t(cm_arf$overall)))
```

```{r arf_plot, echo=FALSE}
#Plot of confusion matrix table
cm_arf_tab <- as.data.frame(cm_arf$table) %>%
  ggplot(aes(Reference, Prediction, fill= Freq)) + 
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1) +
  scale_fill_gradient2(low = "#E7E7E7",
                       mid = "#94B447",
                       high = "#003041") +
  geom_text(aes(label = Freq), color = "white", size = 10) +
  guides(fill = guide_colorbar(barwidth = 0.5,
                               barheight = 10))
#Plot
cm_arf_tab
```

Based on the accuracy increase with the adjusted Random Forests algorithm, we choose it as our final model to train and test in the larger `training` and `validation` sets.

```{r at_modeling, echo=FALSE}
#Results table
at_modeling %>% knitr::kable()
```

# Results

The final Random Forests algorithm trained on the `training` set and tested on the `validation` set incorporated the `sex`, `age_grp`, `mun`, `ses`, and `area` variables as predictors. The code for the algorithm for the R language 


```{r res, message=FALSE, warning=FALSE, echo=TRUE}
#########################################################
#############      RANDOM FORESTS  MODEL    #############
#########################################################

# Model of the entire training set tested in validation.
# With Age Group instead of age.

#Train model
fit_final <- train(pos ~ 
                     sex +
                     age_grp +
                     mun +
                     ses +
                     area,
                   method = "rf",
                   data = training)

#Predict
pred_final <- predict(fit_final, validation)

#Confusion matrix
cm_final <- confusionMatrix(data = pred_final, reference = validation$pos)
```

This model gave us a final accuracy of `0.6727` which meant an increase of `4.5%` over the original baseline model. 
```{r res_tab, echo=FALSE}
#Table of confusion matrix statistics
knitr::kable(as.data.frame(t(cm_final$overall)))
```

```{r res_plot, echo=FALSE}
#Plot of confusion matrix table
cm_final_tab <- as.data.frame(cm_final$table) %>%
  ggplot(aes(Reference, Prediction, fill= Freq)) + 
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1) +
  scale_fill_gradient2(low = "#E7E7E7",
                       mid = "#94B447",
                       high = "#003041") +
  geom_text(aes(label = Freq), color = "white", size = 10) +
  guides(fill = guide_colorbar(barwidth = 0.5,
                               barheight = 10))
#Plot
cm_final_tab
```

It appears that sociodemographic variable might have a limited power as sole predictors of a person's opinion on a given issue. However with a convination of variables and adjustments we were able to predict a participant's opinion accurately almost `70%` of the times, which we believe is enough to partially demonstrate our hypothesis.

```{r res_fin, echo=FALSE}
#Extract accuracy
acc_final <- confusionMatrix(data = pred_final, reference = validation$pos)$overall["Accuracy"]


#Accuracy table
at_final <- tibble(Method="Random Forests",
                   Accuracy = acc_final)
#Results
at_final %>% knitr::kable()
```

# Conclusion

The increase in violence and the high levels of crime in Mexico certainly have changed the perceptions of security among its citizens. However, it appears that the popular perceptions of security can be influenced by many other factors. 

Perceptions of security, while certainly determined by crime, victimization and violence; are indeed influenced by other factors, and that they have a different impact across segments of population. As stated above, far from trying to find those "hidden" more nuanced factors, we found that by simply using some sociodemographic variables we could accurately predict the POS of a respondent in almost 70% of the times, even without considering variables of crime victimization or crime rates.

This, of course doesn't mean that those external factors does not influence POS. On the contrary, those are the main drivers. However, what we try to demonstrate is that despite such factors, perceptions follow a pattern that might also be strongly influenced by other social factors.

# References

UNODC. 2014. Global Study on Homicide 2013. United Nations Office on Drugs and Crime. Vienna: United Nations.

Van Dijk, Jan. 2008. The World of Crime. Thousand Oaks: Sage.

INEGI. 2020. Encuesta Nacional de Victimización y Percepción sobre Seguridad Pública. <https://www.inegi.org.mx/contenidos/programas/envipe/2020/datosabiertos/conjunto_de_datos_envipe2020_csv.zip>

Justice in Mexico. 2016. Public’s perception of security in Mexico stays same despite rise in homicides. San Diego: Justice in Mexico. <https://justiceinmexico.org/publics-perception-of-security-in-mexico-stays-same-despite-rise-in-homicides/>